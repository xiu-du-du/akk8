# http协议
### 服务器渲染：在服务器上直接把数据和html整合在一起，统一返回给浏览器
### 客户端渲染：第一次请求返回一个heml框架，第二次请求拿到html数据。
### 区别：服务器渲染在页面源代码中能看到数据，客户端渲染在源代码中看不到数据
### 解决方法：使用浏览器抓包工具寻找抓取数据的url

### 请求：
#### 请求行 --> 请求方式GET、POST  请求url地址、协议
`General:`
```
 Request URL: https://movie.douban.com/j/chart/top_list?type=24&interval_id=100%3A90&action=&start=0&limit=20
 Request Method: GET
 Status Code: 200 OK
```
##### 请求方式：
1. GET：显示提交
2. POSt：隐式提交

#### 请求头 --> 放一些服务器要使用的附加信息
`Request Headers:`
```
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36
```
##### 请求头常见的一些重要内容
- **user-Agent**:请求载体的身份标识(用啥发送的请求)
- Referer：防盗链(这次请求是从哪个页面来的？反爬用到)
- cookie：本地字符串数据信息(用户登录信息，反爬的token)

#### 请求体 --> 一般放一些请求参数.比如：周杰伦



### 响应：
#### 状态行 --> 协议 状态码 例如：200,302,404等
#### 响应头 --> 放一些客户端要使用的一些附加信息
`Response Headers:`
```
X-Content-Type-Options: nosniff
X-DAE-App: movie
X-DAE-Instance: default
X-Douban-Mobileapp: 0
X-Xss-Protection: 1; mode=block
```
##### 响应头常见的一些重要内容：
- cookie:本地字符串数据信息(用户登录信息，反爬的token)
- 各种神奇的莫名其妙的字符串(这个需要经验，一般都是token字样，防止各种攻击和反爬)

#### 响应体 --> 服务器返回的真正客户端要用的内容(html、json)等


# requests 模块 入门学习
### 安装requests模块
`用清华源下载:
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple requests`
````python
import requests    # 引入requests模块
query = input('输入要搜索的关键词：')
url = f'https://www.sogou.com/web?query={query}'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36'}    # 使用请求头伪装
resp = requests.get(url, headers=headers)  # 请求连接
print(resp)   # 返回状态码200
print(resp.text)  # 取返回文本
# 访问完后记得resp.close()关闭，如果不关闭会堵死
resp.close() #关闭resp 
````

### 百度翻译
#### 检查F12，打开filter，输入dog 找到sug的数据，查看请求url为POST请求，POST请求一般都有`Form Data`，查看里面的参数
````python
import requests
url = 'https://fanyi.baidu.com/sug'
s=input('请输入要翻译的英文单词：')
data = {
    'kw': s
}
# 发送post请求，发送的数据必须放在字典中，通过data参数进行发送
resp = requests.post(url, data=data)
print(resp)
print(resp.json())   # 将服务器返回的内容直接处理成json()  => dict字典
# 访问完后记得resp.close()关闭，如果不关闭会堵死
resp.close() #关闭resp 
````

### 豆瓣排行榜
#### 检查F12，打开filter，`xhr: ajax请求`，`url链接中找问号(?)，问号之前是链接，问号之后是参数`，查看请求url为POST请求，POST请求一般都有`Query String Parameters`，查看里面的参数
#### 如果发送的get请求很长，可以用第二种方式封装get的请求参数
````python
import requests
url='https://movie.douban.com/j/chart/top_list'
# 重新封装参数
param={
    'type': '24',
    'interval_id': '100:90',
    'action':'',
    'start': 0,
    'limit': 20,
}
# 获取不到内容，就使用请求头进行伪装后再爬取
headers={
'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36'
}
resp=requests.get(url,params=param,headers=headers)
print(resp)
print(resp.url)
print(resp.json())
# 访问完后记得resp.close()关闭，如果不关闭会堵死
resp.close() #关闭resp 
````

# 正则表达式
### 元字符
````
.   匹配除换行符意外的任意字符
\w  匹配字母或数字或下划线
\s  匹配任意的空白符
\d  匹配数字
\n  匹配一个换行符
\t  匹配一个制表符

^   匹配字符串的开始
$   匹配字符串的结尾

\W  匹配非字母或非数字或非下划线
\S  匹配非空白符的任意字符
\D  匹配非数字的任意字符
a|b 匹配a或b
()  匹配括号内的表达式，也表示一个组
[0-9a-zA-Z] 匹配字符组中的内容
[^0-9a-zA-Z]    匹配非字符组中内容的任意字符
````
### 量词
````
*   重复0次或更多次
+   重复一次或更多次
?   重复0次或一次
{n} 重复n次
{n,}    重复n次或更多次
{n,m}   重复n到m次
````
### 贪婪匹配和惰性匹配
````
.*  贪婪匹配-尽可能多的匹配
.*? 惰性匹配-固定逻辑，尽可能少的匹配内容
````
```pycon
re.findall(r'\d+','我的电话是：10086。你的电话是：10010。')
# 正则表达式前面加个'r'，只有好处没有坏处
# 正则表达式前的r 表示原生字符串（rawstring），该字符串声明了引号中的内容表示该内容的原始含义，避免了多次转义造成的反斜杠困扰。
```
## re模块的使用
> import re
#### 匹配字符串中的内容
````python
# # findall(正则表达式，要匹配的文本):匹配字符串中所有的符合正则的内容[返回的是列表]
# lst=re.findall(r'\d+','我的电话是：10086。你的电话是：10010。')
# print(lst)
# ----------------------------------------------------------------------------------------------------------
# # finditer(正则表达式，要匹配的内容):匹配字符串中所有的内容[返回的是迭代器],从迭代器中拿到内容需要.group()
# it=re.finditer(r'\d+','我的电话是：10086。你的电话是：10010。')
# for i in it:
#     print(i.group())
# ----------------------------------------------------------------------------------------------------------
# # search():找到一个结果就返回，返回的结果是match对象。拿数据需要.group()
# s=re.search(r'\d+','我的电话是：10086。你的电话是：10010。')
# print(s.group())
# ----------------------------------------------------------------------------------------------------------
# # match():从字符串的开头开始匹配。拿数据需要.group()
# s=re.match(r'\d+','我的电话是：10086。你的电话是：10010。')
# print(s.group())
````
#### 预加载
````python
# # 预加载正则表达式
# obj=re.compile(r'\d+')   # 提高效率：先提前加载好正则，等用的时候再用
# ret=obj.finditer('我的电话是：10086。你的电话是：10010。')
# for i in ret:
#     print(i.group())
````

#### 提取出字符串中的内容
````python
# # re.S: 让.能匹配换行符
# # (?P<分组名称>正则) 可以单独从正则匹配的内容中进一步提取内容
# obj=re.compile(r"<div class='(.*?)'><span id='(?P<id>\d+)'>(?P<name>.*?)</span></div>",re.S)   # 提高效率：先提前加载好正则，等用的时候再用
# aa=obj.finditer(ss)
# for i in aa:
#     print(i.group('id'),end='    ')
#     print(i.group('name'))
````

#### csv 模块
> csv文件的特点：字符串，字符串，字符串，字符串......


#### 爬取豆瓣top250电影排行
````python
# 拿到页面源代码。  request模块
# 通过正则来提取有效信息。  re模块
import requests
import re
import csv

num1=0
while num1 <= 250:
    url ="https://movie.douban.com/top250?start="+str(num1)+"&filter="   
    headers= {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36"
        }
    resp=requests.get(url,headers=headers)   # 伪装请求url
    page_content=resp.text   #拿到返回的源代码文本

    # 解析数据
    # 预加载正则
    obj = re.compile(r'<li>.*?<div class="item">.*?<span class="title">(?P<name>.*?)'
                     r'</span>.*?<p class="">.*?<br>.*?(?P<year>\d+)&nbsp.*?<span'
                     r' class="rating_num" property="v:average">(?P<score>.*?)</span>.*?'
                     r'<span>(?P<num>.*?)</span>',re.S)
    # 使用正则匹配拿到的源代码内容
    ret=obj.finditer(page_content)
    # 写到.csv文件
    f=open('data.csv',mode='a',encoding='utf-8')
    csv_writer=csv.writer(f)
    # 遍历获取要拿到的文本
    for i in ret:
        # print(i.group('name'),end='    ')
        # print(i.group('year'),end='    ')
        # print(i.group('score'),end='    ')
        # print(i.group('num'))
        
        # 创建字典接收遍历得到的值
        dic=i.groupdict()
        # 讲最终得到的值写入.csv文件
        csv_writer.writerow(dic.values())
    num1+=25
f.close()
print('ok')
````
#### 电影天堂热片爬取
> verify=False  把https安全改成http
````python
# 1. 定位到2021必看热片
# 2. 从2021必看热片中提取到子页面的链接地址
# 3. 请求子页面的链接地址，拿到我们想要的下载地址......

import requests
import re
domain='https://dytt89.com/'
resp=requests.get(domain,verify=False)   # verify=False去掉安全验证
resp.encoding='gb2312'  # 也可以写gbk，gbk和gb2312都是国标

# 拿到ul里的li
# 首页正则，找出列表ul
obj1=re.compile(r"2021必看热片.*?<ul>(?P<ul>.*?)</ul>",re.S)
# ul列表正则，找出ul列表里的子链接
obj2=re.compile(r"<a href='(?P<href>.*?)'")
# 子页面正则，找出电影下载链接和片名
obj3 = re.compile(r'◎片　　名　(?P<name>.*?)<br />.*?<td style="WORD-WRAP: br'
                  r'eak-word" bgcolor="#fdfddf"><a href="(?P<down>.*?)">', re.S)

# 匹配主页面2021必看热片里的ul列表，获取到ul的返回源代码
result1=obj1.finditer(resp.text)
# 设置一个空列表用来存放子页面链接
child_href_list=[]
# 遍历主页面里ul的数据
for it in result1:
    ul=it.group('ul')
    result2=obj2.finditer(ul)  # 匹配ul里子页面链接
    # 遍历ul里匹配到的子页面链接
    for itt in result2:
        href=itt.group('href')   # 拿到子页面链接的数据
        child_href=domain+href.strip('/')   # 拼接主域名
        child_href_list.append(child_href)  # 将完整的子页面链接存放到列表中
# 提取子页面内容
# 遍历列表中存放的子页面链接
for href in child_href_list:
    child_resp=requests.get(href,verify=False)   # 请求每个子页面链接
    child_resp.encoding='gb2312'   # 设置编码
    result3=obj3.finditer(child_resp.text)   # 匹配每个子页面中的下载地址和片名
    # 遍历匹配到的下载地址和片名
    for i in result3:
        down=i.group('down')   # 获取到下载地址
        name=i.group('name')   # 获取到片名
        print(name)
        print(down)
        print('===')
````

### bs4解析

#### 了解html

`语法：<标签 属性=“属性值”>标记内容</标签>`

>  html 提供了非常多标签，每个标签都是用标记的形式显示在页面。

> 标签都具有多个属性，例如：<h1 align="center"></h1>

#### 安装bs4模块

> pip install bs4

````python
# pip install bs4

# 1. 拿到页面源代码
# 2. 使用bs4 进行解析
import csv
import requests
from bs4 import BeautifulSoup
url = 'http://xinfadi.com.cn/getPriceData.html'
resp=requests.get(url)   # 获取页面源代码
text=resp.json()   # 获取菜品json
f=open('caijia.csv',mode='w',encoding='utf-8')
csv_writer=csv.writer(f)
for i in text['list']:   # 遍历json键值‘list’内的数据
    prodName=i['prodName']  # 打印输出key:‘proName’的值
    lowPrice=i['lowPrice']
    avgPrice=i['avgPrice']
    highPrice=i['highPrice']
    pubDate=i['pubDate']
    csv_writer.writerow([prodName,lowPrice,avgPrice,highPrice,pubDate])   # 写到csv文件
print(type(text))
f.close()
# =====================================================================
# 网站已改版，下面方法过期。
# =====================================================================
# 1. 把页面源代码交给BeautifulSoup()进行处理，生产bs对象
# page=BeautifulSoup(resp.text,'html.parser')   # 指定html
# 2. 从bs对象中查找数据
# find(标签，属性=值) 和 find_all(标签，属性=值)    find只找第一个，find_all全部找出来
# table=page.find("table",class_="hq_table")   #class是python中的关键字，加_为了区分
# table=page.find("table",attrs={"class":"hq_table"})    # 和上一行是一个意思，避免class
# 拿到所有数据行
# trs =table.find_all("tr")[1:]   #不拿表格的标题，所以设置从1开始往后拿
# for tr in trs:
#     tds=tr.find_all("td")   # 拿到每行中的所有td
#     name=tds[0].text   # .text 表示拿到被标签标记的内容
#     low=tds[1].text   # .text 表示拿到被标签标记的内容
#     avg=tds[2].text   # .text 表示拿到被标签标记的内容
#     high=tds[3].text   # .text 表示拿到被标签标记的内容
#     gui=tds[4].text   # .text 表示拿到被标签标记的内容
#     kind=tds[5].text   # .text 表示拿到被标签标记的内容
#     date=tds[5].text   # .text 表示拿到被标签标记的内容
#     csv_writer.writerow([name,low,avg,high,gui,kind,date])
# f.close()
# print("ok")
````

